{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0cQkQcvO_U7"
      },
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ⭐ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐\n",
        "</div>\n",
        "\n",
        "To install Unsloth your local device, follow [our guide](https://docs.unsloth.ai/get-started/install-and-update). This notebook is licensed [LGPL-3.0](https://github.com/unslothai/notebooks?tab=LGPL-3.0-1-ov-file#readme).\n",
        "\n",
        "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Pz2zn7dO_U8"
      },
      "source": [
        "### News"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVt0Kma4O_U8"
      },
      "source": [
        "\n",
        "Unsloth's [Docker image](https://hub.docker.com/r/unsloth/unsloth) is here! Start training with no setup & environment issues. [Read our Guide](https://docs.unsloth.ai/new/how-to-train-llms-with-unsloth-and-docker).\n",
        "\n",
        "[gpt-oss RL](https://docs.unsloth.ai/new/gpt-oss-reinforcement-learning) is now supported with the fastest inference & lowest VRAM. Try our [new notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-GRPO.ipynb) which creates kernels!\n",
        "\n",
        "Introducing [Vision](https://docs.unsloth.ai/new/vision-reinforcement-learning-vlm-rl) and [Standby](https://docs.unsloth.ai/basics/memory-efficient-rl) for RL! Train Qwen, Gemma etc. VLMs with GSPO - even faster with less VRAM.\n",
        "\n",
        "Unsloth now supports Text-to-Speech (TTS) models. Read our [guide here](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning).\n",
        "\n",
        "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBtJo4qaO_U8"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvWlLVzrcGzI",
        "outputId": "de88c800-da0e-4edf-c416-a2e95db192df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sC0GzX_vO_U8"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os, re\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    import torch; v = re.match(r\"[0-9\\.]{3,}\", str(torch.__version__)).group(0)\n",
        "    xformers = \"xformers==\" + (\"0.0.32.post2\" if v == \"2.8.0\" else \"0.0.29.post3\")\n",
        "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "!pip install transformers==4.56.2\n",
        "!pip install --no-deps trl==0.22.2\n",
        "!pip install --upgrade wandb\n",
        "!pip install modelscope"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjxOimi5P8PC",
        "outputId": "b427ac55-fe16-4fe1-cdc7-6e10cf357827"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: modelscope in /usr/local/lib/python3.12/dist-packages (1.33.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from modelscope) (3.20.0)\n",
            "Requirement already satisfied: requests>=2.25 in /usr/local/lib/python3.12/dist-packages (from modelscope) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from modelscope) (75.2.0)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.12/dist-packages (from modelscope) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.26 in /usr/local/lib/python3.12/dist-packages (from modelscope) (2.5.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.25->modelscope) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.25->modelscope) (3.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.25->modelscope) (2025.11.12)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.12/importlib/__init__.py:131: UserWarning: WARNING: Unsloth should be imported before [trl, transformers, peft] to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
            "\n",
            "Please restructure your imports with 'import unsloth' at the top of your file.\n",
            "  _bootstrap._exec(spec, module)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: WARNING `trust_remote_code` is True.\n",
            "Are you certain you want to do remote code execution?\n",
            "==((====))==  Unsloth 2026.1.2: Fast Nemotron patching. Transformers: 4.56.2.\n",
            "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.557 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 8.0. CUDA Toolkit: 12.6. Triton: 3.5.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ],
      "source": [
        "# %%\n",
        "!pip install modelscope\n",
        "import os\n",
        "import importlib\n",
        "\n",
        "# Set environment variable BEFORE importing or reloading unsloth\n",
        "os.environ['UNSLOTH_USE_MODELSCOPE'] = '1'\n",
        "\n",
        "import unsloth\n",
        "\n",
        "import multiprocessing as mp\n",
        "\n",
        "#os.environ[\"HF_DATASETS_DISABLE_MULTIPROCESSING\"] = \"1\"\n",
        "#os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "# For non-moe models use FastLanguageModel!\n",
        "from unsloth import FastModel\n",
        "import torch\n",
        "\n",
        "\n",
        "# GLOBAL CONFIG VARIABLES\n",
        "input_model = \"unsloth/Qwen3-30B-A3B-Thinking-2507\"\n",
        "output_model_name = \"TeichAI/Qwen3-30B-A3B-Thinking-Claude-4.5-Opus-High-Reasoning-Distill\"\n",
        "dataset_id = \"TeichAI/claude-4.5-opus-high-reasoning-250x\"\n",
        "hf_token = \"hf_...\"\n",
        "\n",
        "### Start data prep, model loading and training ###\n",
        "\n",
        "model, tokenizer = FastModel.from_pretrained(\n",
        "    model_name=input_model,\n",
        "    max_seq_length=8192,  # Context length - can be longer, but uses more memory\n",
        "    load_in_4bit=True,  # 4bit uses much less memory\n",
        "    load_in_8bit=False,  # A bit more accurate, uses 2x memory\n",
        "    full_finetuning=False,  # We have full finetuning now!\n",
        "    token=hf_token,\n",
        "    trust_remote_code=True, # Enable custom model code loading\n",
        ")\n",
        "\n",
        "model = FastModel.get_peft_model(\n",
        "    model,\n",
        "    r=32,  # Choose any number > 0! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "    ],\n",
        "    lora_alpha=32,  # Best to choose alpha = rank or rank*2\n",
        "    lora_dropout=0,  # Supports any, but = 0 is optimized\n",
        "    bias=\"none\",  # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for very long context\n",
        "    random_state=3407,\n",
        "    use_rslora=False,  # We support rank stabilized LoRA\n",
        "    loftq_config=None,  # And LoftQ\n",
        ")\n",
        "\n",
        "raw_dataset = load_dataset(dataset_id, split=\"train\")\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    convos = examples[\"messages\"]\n",
        "    texts = [\n",
        "        tokenizer.apply_chat_template(\n",
        "            convo, tokenize=False, add_generation_prompt=False\n",
        "        )\n",
        "        for convo in convos\n",
        "    ]\n",
        "    return {\"text\": texts}\n",
        "\n",
        "\n",
        "train_dataset = raw_dataset.map(formatting_prompts_func, batched=True)\n",
        "\n",
        "\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "mp.freeze_support()\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=None,\n",
        "    dataset_num_proc=1,  # <- disable multiprocessing on Windows\n",
        "    args=SFTConfig(\n",
        "        dataset_text_field=\"text\",\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=5,\n",
        "        #num_train_epochs=1,\n",
        "        max_steps=750,\n",
        "        learning_rate=2e-4,\n",
        "        logging_steps=1,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3447,\n",
        "        report_to=\"none\",\n",
        "        dataloader_num_workers=0,\n",
        "        output_dir=f\"/content/drive/MyDrive/Colab/{output_model_name}\",\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=10,\n",
        "        save_total_limit=2000,\n",
        "    ),\n",
        ")\n",
        "# %%\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
        "\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "model.config.eos_token_id = tokenizer.eos_token_id\n",
        "model.config.bos_token_id = tokenizer.bos_token_id\n",
        "model.generation_config.eos_token_id = tokenizer.eos_token_id\n",
        "model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
        "model.generation_config.bos_token_id = tokenizer.bos_token_id\n",
        "\n",
        "trainer_stats = trainer.train(resume_from_checkpoint=False)\n",
        "\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f422JgM9sdVT"
      },
      "source": [
        "### Saving to float16 for VLLM\n",
        "\n",
        "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZgcJIhJ0I_es"
      },
      "outputs": [],
      "source": [
        "if True: # Pushing to HF Hub\n",
        "    model.push_to_hub_merged(\n",
        "        \"TeichAI/Nemotron-3-Nano-30B-A3B-Claude-4.5-Opus-High-Reasoning-Distill\",\n",
        "        tokenizer,\n",
        "        save_method = \"merged_16bit\",\n",
        "        token = hf_token\n",
        "      )\n",
        "\n",
        "if True: # Pushing to HF Hub\n",
        "    model.push_to_hub_gguf(\n",
        "        \"TeichAI/Nemotron-3-Nano-30B-A3B-Claude-4.5-Opus-High-Reasoning-Distill-GGUF\",\n",
        "        tokenizer,\n",
        "        quantization_method = [\"bf16\", \"q8_0\", \"q4_0\"],\n",
        "        token = hf_token\n",
        "      )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
